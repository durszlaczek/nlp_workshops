{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Games, news - context matters!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim, string\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "news = pd.read_csv('../data/news_lang.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>score</th>\n",
       "      <th>num_comments</th>\n",
       "      <th>created_utc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Texas teen tackled by cop at pool party files ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1483629018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>John Kerry In Leaked Audio Admits U.S. Allowed...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1483872751</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2017 American Liberty 225th Anniversary Gold C...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1484327875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Repair broken glass with Sensible cost | Call ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1484798494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Today is the last day to register for Obamacare</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1485872414</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  score  num_comments  \\\n",
       "0  Texas teen tackled by cop at pool party files ...      0             0   \n",
       "1  John Kerry In Leaked Audio Admits U.S. Allowed...      0             0   \n",
       "2  2017 American Liberty 225th Anniversary Gold C...      0             0   \n",
       "3  Repair broken glass with Sensible cost | Call ...      0             0   \n",
       "4    Today is the last day to register for Obamacare      0             0   \n",
       "\n",
       "   created_utc  \n",
       "0   1483629018  \n",
       "1   1483872751  \n",
       "2   1484327875  \n",
       "3   1484798494  \n",
       "4   1485872414  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization\n",
    "\n",
    "`gensims`'s `Word2Vec` as an argument takes a corpus as a list of lists of tokens. Let's prepare the tokenized form of the corpus we have.\n",
    "\n",
    "A `RegexpTokenizer` (from `nltk`) splits a string into substrings using a regular expression. For example, the following tokenizer forms tokens out of alphabetic and numeric sequences. It splits words on *most* punctuation marks. It keeps acronyms and numerics unsplitted (e.g. *U.S.A.* will not be splitted).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "pattern = r'[\\d.,]+|[A-Z][.A-Z]+\\b\\.*|\\w+|\\S'\n",
    "tokenizer = RegexpTokenizer(pattern)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['U.S.A',\n",
       " 'Count',\n",
       " 'U.S.A.',\n",
       " 'Sec',\n",
       " '.',\n",
       " 'of',\n",
       " 'U.S.',\n",
       " 'Name',\n",
       " ':',\n",
       " 'Dr',\n",
       " '.',\n",
       " 'John',\n",
       " 'Doe',\n",
       " 'J.',\n",
       " 'Doe',\n",
       " ',',\n",
       " '1.11',\n",
       " '1,000',\n",
       " '10',\n",
       " '-',\n",
       " '-',\n",
       " '20',\n",
       " '10',\n",
       " '-',\n",
       " '20']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "line = \"U.S.A Count U.S.A. Sec.of U.S. Name:Dr.John Doe J.Doe, 1.11 1,000 10--20 10-20\"\n",
    "tokenizer.tokenize(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_corpus(sentences, tok):\n",
    "    tok_sentences = [tok.tokenize(x) for x in sentences]\n",
    "    return [[x.lower() for x in y if x not in string.punctuation] for y in tok_sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_corpus = prepare_corpus(news['title'], tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['texas',\n",
       " 'teen',\n",
       " 'tackled',\n",
       " 'by',\n",
       " 'cop',\n",
       " 'at',\n",
       " 'pool',\n",
       " 'party',\n",
       " 'files',\n",
       " 'lawsuit']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_corpus[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_model = gensim.models.Word2Vec(news_corpus,\n",
    "                                    sg=0, # CBOW vs. skip-gram\n",
    "                                    size=100, # feature vectors' length\n",
    "                                    window=5, # window size\n",
    "                                    min_count=1, # ignore all words with total frequency lower than this\n",
    "                                    negative=1, # if set to 0, no negative samping is used\n",
    "                                    seed=123\n",
    "                                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6634550511503782"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_model.wv.similarity('poland', 'u.s.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6995020961367224"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_model.wv.similarity('poland', 'czech')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.12151777113689202"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_model.wv.similarity('poland', 'dog')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('iraq', 0.9142078757286072),\n",
       " ('yemen', 0.9018725156784058),\n",
       " ('syria', 0.8834158778190613),\n",
       " ('raqqa', 0.8758918642997742),\n",
       " ('iraqi', 0.8730102777481079),\n",
       " ('mosul', 0.8689094185829163),\n",
       " ('libya', 0.8657315373420715),\n",
       " ('afghan', 0.8612012267112732),\n",
       " ('kirkuk', 0.8582304120063782),\n",
       " ('myanmar', 0.8478946089744568)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_model.wv.most_similar('afghanistan')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('actress', 0.8610506057739258),\n",
       " ('mary', 0.8366692066192627),\n",
       " ('playboy', 0.8277504444122314),\n",
       " ('bryan', 0.8207701444625854),\n",
       " ('sir', 0.8192210793495178),\n",
       " ('chris', 0.8170576691627502),\n",
       " ('filmmaker', 0.813789963722229),\n",
       " ('singer', 0.8101747632026672),\n",
       " ('songwriter', 0.7951234579086304),\n",
       " ('jerry', 0.7950710654258728)]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_model.wv.most_similar(positive=['woman', 'actor'], negative=['man'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('prince', 0.8167217969894409),\n",
       " ('drake', 0.8065531253814697),\n",
       " ('stephanie', 0.7968810796737671),\n",
       " ('queen', 0.7942199110984802),\n",
       " ('sajal', 0.785223126411438),\n",
       " ('kristen', 0.7827389240264893),\n",
       " ('deepika', 0.7776816487312317),\n",
       " ('stewart', 0.7765840291976929),\n",
       " ('middleton', 0.774808406829834),\n",
       " ('beyonce', 0.7734202146530151)]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_model.wv.most_similar(positive=['woman', 'king'], negative=['man'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "games = pd.read_csv('../data/gaming.csv')\n",
    "games_corpus = prepare_corpus(games['title'], tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "games_model = gensim.models.Word2Vec(games_corpus,\n",
    "                                     min_count=1,\n",
    "                                     seed=123\n",
    "                                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mouse\n",
      "Games: keyboard, monitor, headphones, router, headset, laptop, chair, headsets, usb, desktop\n",
      "News: colorful, bouquet, lip, tubs, perfume, alluring, bra, workout, patterns, clothes\n",
      "\n",
      "bomb\n",
      "Games: bush, shotgun, sohjy, raiding, bow, dirty, quad, nuke, uzi, spawn\n",
      "News: deadly, drill, incident, attack, strikes, damascus, flooding, ship, blast, wwii\n",
      "\n",
      "blood\n",
      "Games: knights, harvest, titan, osiris, motoi, flashpoint, lvl2, chaos, fusion, rush\n",
      "News: obesity, drugs, patient, cancer, ptsd, organs, bees, heroin, body, disease\n",
      "\n",
      "war\n",
      "Games: mordor, wardayz, annihilation, snowzilla, warzeib, morose, warcraft, fight3, tomrrow, okage\n",
      "News: nuclear, preemptive, peace, syria, moab, threat, democracy, preparing, iran, ukraine\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def print_most_similar(word, top_n=10):\n",
    "    games_set = games_model.wv.most_similar(word, topn=top_n)\n",
    "    news_set = news_model.wv.most_similar(word, topn=top_n)\n",
    "    games_set = [x[0] for x in games_set]\n",
    "    news_set = [x[0] for x in news_set]\n",
    "    print(word)\n",
    "    print('Games:', ', '.join(games_set))\n",
    "    print('News:', ', '.join(news_set))\n",
    "    print()\n",
    "\n",
    "    \n",
    "print_most_similar('mouse')\n",
    "print_most_similar('bomb')\n",
    "print_most_similar('blood')\n",
    "print_most_similar('war')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp-workshop",
   "language": "python",
   "name": "nlp-workshop"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
