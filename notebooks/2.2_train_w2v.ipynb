{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Games, news - context matters!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim, string \n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "news = pd.read_csv('../data/news_lang.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>score</th>\n",
       "      <th>num_comments</th>\n",
       "      <th>created_utc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Texas teen tackled by cop at pool party files ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1483629018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>John Kerry In Leaked Audio Admits U.S. Allowed...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1483872751</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2017 American Liberty 225th Anniversary Gold C...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1484327875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Repair broken glass with Sensible cost | Call ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1484798494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Today is the last day to register for Obamacare</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1485872414</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  score  num_comments  \\\n",
       "0  Texas teen tackled by cop at pool party files ...      0             0   \n",
       "1  John Kerry In Leaked Audio Admits U.S. Allowed...      0             0   \n",
       "2  2017 American Liberty 225th Anniversary Gold C...      0             0   \n",
       "3  Repair broken glass with Sensible cost | Call ...      0             0   \n",
       "4    Today is the last day to register for Obamacare      0             0   \n",
       "\n",
       "   created_utc  \n",
       "0   1483629018  \n",
       "1   1483872751  \n",
       "2   1484327875  \n",
       "3   1484798494  \n",
       "4   1485872414  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization\n",
    "\n",
    "`gensims`'s `Word2Vec` as an argument takes a corpus as a list of lists of tokens. Let's prepare the tokenized form of the corpus we have.\n",
    "\n",
    "A `RegexpTokenizer` (from `nltk`) splits a string into substrings using a regular expression. For example, the following tokenizer forms tokens out of alphabetic and numeric sequences. It splits words on *most* punctuation marks. It keeps acronyms and numerics unsplitted (e.g. *U.S.A.* will not be splitted).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "pattern = r'[\\d.,]+|[A-Z][.A-Z]+\\b\\.*|\\w+|\\S'\n",
    "tokenizer = RegexpTokenizer(pattern)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['U.S.A',\n",
       " 'Count',\n",
       " 'U.S.A.',\n",
       " 'Sec',\n",
       " '.',\n",
       " 'of',\n",
       " 'U.S.',\n",
       " 'Name',\n",
       " ':',\n",
       " 'Dr',\n",
       " '.',\n",
       " 'John',\n",
       " 'Doe',\n",
       " 'J.',\n",
       " 'Doe',\n",
       " ',',\n",
       " '1.11',\n",
       " '1,000',\n",
       " '10',\n",
       " '-',\n",
       " '-',\n",
       " '20',\n",
       " '10',\n",
       " '-',\n",
       " '20']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "line = \"U.S.A Count U.S.A. Sec.of U.S. Name:Dr.John Doe J.Doe, 1.11 1,000 10--20 10-20\"\n",
    "tokenizer.tokenize(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_corpus(sentences, tok):\n",
    "    tok_sentences = [tok.tokenize(x) for x in sentences]\n",
    "    return [[x.lower() for x in y if x not in string.punctuation] for y in tok_sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_corpus = prepare_corpus(news['title'], tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['texas',\n",
       " 'teen',\n",
       " 'tackled',\n",
       " 'by',\n",
       " 'cop',\n",
       " 'at',\n",
       " 'pool',\n",
       " 'party',\n",
       " 'files',\n",
       " 'lawsuit']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_corpus[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_model = gensim.models.Word2Vec(news_corpus,\n",
    "                                    sg=0, # CBOW vs. skip-gram\n",
    "                                    size=100, # feature vectors' length\n",
    "                                    window=5, # window size\n",
    "                                    min_count=1, # ignore all words with total frequency lower than this\n",
    "                                    negative=1, # if set to 0, no negative samping is used\n",
    "                                    seed=123\n",
    "                                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.658789823696318"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_model.wv.similarity('poland', 'u.s.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6328641246562836"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_model.wv.similarity('poland', 'czech')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1941660502315914"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_model.wv.similarity('poland', 'dog')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('raqqa', 0.9185310006141663),\n",
       " ('iraq', 0.9065133333206177),\n",
       " ('yemen', 0.9029085636138916),\n",
       " ('libya', 0.896985650062561),\n",
       " ('syria', 0.892918050289154),\n",
       " ('mosul', 0.8667630553245544),\n",
       " ('syrian', 0.8576147556304932),\n",
       " ('tehran', 0.8539131283760071),\n",
       " ('troops', 0.8490171432495117),\n",
       " ('taliban', 0.8487639427185059)]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_model.wv.most_similar('afghanistan')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('actress', 0.8733424544334412),\n",
       " ('mary', 0.8640809655189514),\n",
       " ('singer', 0.8306462168693542),\n",
       " ('chris', 0.8297317624092102),\n",
       " ('columnist', 0.8273946046829224),\n",
       " ('playboy', 0.8151363730430603),\n",
       " ('filmmaker', 0.8150051832199097),\n",
       " ('ian', 0.8096398115158081),\n",
       " ('chopra', 0.8077244162559509),\n",
       " ('comedian', 0.8040573596954346)]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_model.wv.most_similar(positive=['woman', 'actor'], negative=['man'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('queen', 0.8345198035240173),\n",
       " ('arjun', 0.8182545900344849),\n",
       " ('noor', 0.8052791953086853),\n",
       " ('dave', 0.7958756685256958),\n",
       " ('alexander', 0.795805811882019),\n",
       " ('dutt', 0.7911546230316162),\n",
       " ('kate', 0.7893512845039368),\n",
       " ('kerr', 0.7872197031974792),\n",
       " ('emma', 0.7852388620376587),\n",
       " ('crown', 0.7845532298088074)]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_model.wv.most_similar(positive=['woman', 'king'], negative=['man'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "games = pd.read_csv('../data/gaming.csv')\n",
    "games_corpus = prepare_corpus(games['title'], tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "games_model = gensim.models.Word2Vec(games_corpus,\n",
    "                                     min_count=1,\n",
    "                                     seed=123\n",
    "                                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mouse\n",
      "Games: keyboard, headphones, headset, monitor, laptop, headsets, usb, chair, desktop, router\n",
      "News: color, cake, workout, lip, gloss, bra, fashionable, fabulous, makeup, trendy\n",
      "\n",
      "bomb\n",
      "Games: spawn, bullets, raiding, chopper, camping, dirty, crossbow, grenade, shotgun, bullet\n",
      "News: deadly, attack, incident, syrian, flooding, strikes, drill, weapons, ship, weapon\n",
      "\n",
      "blood\n",
      "Games: knights, wine, chaos, harvest, dragons, playthrough, wonderboy, flashpoint, temple, handcarved\n",
      "News: patient, alcohol, cancer, disease, epilepsy, body, organs, depression, failure, cells\n",
      "\n",
      "war\n",
      "Games: mordor, wardayz, morose, unrest, warcraft, hydro, cutscreen, snowzilla, earth, vegalta\n",
      "News: nuclear, syria, democracy, moab, divided, threat, iran, preemptive, dprk, regime\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def print_most_similar(word, top_n=10):\n",
    "    games_set = games_model.wv.most_similar(word, topn=top_n)\n",
    "    news_set = news_model.wv.most_similar(word, topn=top_n)\n",
    "    games_set = [x[0] for x in games_set]\n",
    "    news_set = [x[0] for x in news_set]\n",
    "    print(word)\n",
    "    print('Games:', ', '.join(games_set))\n",
    "    print('News:', ', '.join(news_set))\n",
    "    print()\n",
    "\n",
    "    \n",
    "print_most_similar('mouse')\n",
    "print_most_similar('bomb')\n",
    "print_most_similar('blood')\n",
    "print_most_similar('war')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
